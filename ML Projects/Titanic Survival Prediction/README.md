# ğŸ›³ï¸ Titanic Survival Prediction â€“ Machine Learning Project

## ğŸ“Œ Overview
This project predicts passenger survival during the **Titanic disaster** using **machine learning techniques**.  
It demonstrates a complete **end-to-end ML pipeline**, including data preprocessing, feature engineering, model comparison, and evaluation.

The final model achieves **~84% accuracy**, making it reliable and interview-ready.

---

## ğŸ¯ Problem Statement
Build a **binary classification model** to predict whether a passenger survived the Titanic disaster.

- **Target Variable:** `Survived`
  - `1` â†’ Survived
  - `0` â†’ Did Not Survive

---

## ğŸ“Š Dataset Information
- **Source:** Kaggle â€“ *Titanic: Machine Learning from Disaster*
- **Total Records:** 891
- **Type:** Structured tabular dataset

### Key Features
- Pclass â€“ Passenger class  
- Sex â€“ Gender  
- Age â€“ Age of passenger  
- SibSp â€“ Siblings/Spouses aboard  
- Parch â€“ Parents/Children aboard  
- Fare â€“ Ticket fare  
- Embarked â€“ Port of embarkation  

---

## ğŸ› ï¸ Data Preprocessing
- Removed low-importance columns (`Cabin`, `Ticket`, `PassengerId`)
- Handled missing values using **group-based median imputation**
- Filled categorical missing values using **mode**
- Applied **One-Hot Encoding** for categorical features

---

## âš™ï¸ Feature Engineering
- **Title extraction** from passenger names (Mr, Mrs, Miss, Rare)
- **FamilySize** feature
- **IsAlone** indicator
- Age imputation based on **Sex & Passenger Class**

These steps significantly improved model performance.

---

## ğŸ¤– Models Implemented
| Model | Accuracy |
|------|---------|
| Logistic Regression | ~78% |
| Random Forest (Tuned) | ~82â€“83% |
| Gradient Boosting | **~83â€“84%** |

---

## ğŸ† Best Model
- **Gradient Boosting Classifier**
- **Final Accuracy:** ~84%
- Robust and effective for non-linear relationships

---

## ğŸ“ˆ Evaluation Metrics
- Accuracy
- Precision
- Recall
- F1-Score
- Confusion Matrix

---

## ğŸ§  Key Learnings
- Feature engineering greatly improves accuracy
- Ensemble models outperform baseline models
- Clean preprocessing is crucial for ML success

---

## ğŸ› ï¸ Tech Stack
- Python
- Pandas, NumPy
- Matplotlib, Seaborn
- Scikit-learn
- Jupyter Notebook

---

## ğŸš€ How to Run
Install dependencies:
```
pip install pandas numpy matplotlib seaborn scikit-learn
```

Run the notebook:
```
Titanic.ipynb
```

---

## ğŸ“ Dataset Reference
- Kaggle Titanic Dataset

---

â­ *This project is ideal for ML portfolios, GitHub, Kaggle, and interviews.*
